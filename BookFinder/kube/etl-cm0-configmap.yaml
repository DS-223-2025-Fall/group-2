apiVersion: v1
data:
  .env: |
    DATABASE_URL=postgresql+psycopg2://postgres:postgres@db:5432/postgres
    DB_USER=postgres
    DB_PASSWORD=postgres
    DB_NAME=db
    PGADMIN_EMAIL=admin@admin.com
    PGADMIN_PASSWORD=admin
  __init__.py: ""
  Dockerfile: "FROM python:3.10-slim-bullseye\n\nRUN apt-get update && apt-get install -y \\\n    build-essential libpq-dev libfreetype6-dev libpng-dev libjpeg-dev \\\n    libblas-dev liblapack-dev gfortran \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Set working directory\nWORKDIR /etl\n\n# Copy requirements file and install dependencies\nCOPY requirements.txt . \nRUN pip3 install --upgrade pip\nRUN pip3 install -r requirements.txt\n\n# Copy the rest of the application code\nCOPY . .\n\n# Expose port 3000\nEXPOSE 3000\n\n# Command to run the python file\nCMD [\"python\", \"etl_process.py\"]"
  etl_process.py: "import pandas as pd\nimport glob\nimport os\nfrom os import path\nfrom sqlalchemy import text\nfrom sqlalchemy.orm import sessionmaker\nimport logging\nfrom database.database import engine\n\nlogger = logging.getLogger(__name__)\n\ndef load_csv_to_table(table_name: str, csv_path: str) -> None:\n    \"\"\"\n    Load data from a CSV file into a database table.\n    \"\"\"\n    df = pd.read_csv(csv_path)\n    \n    # Map CSV columns to DB table columns\n    column_mapping = {\n        \"ISBN\": \"isbn\",\n        \"title\": \"title\",\n        \"author\": \"author\",\n        \"genre\": \"genre\",\n        \"language\": \"language\",\n        \"data_source\": \"data_source\",\n        \"url\": \"description\"  # optional: store URL as description\n    }\n\n    df = df.rename(columns=column_mapping)\n    \n    # Keep only the columns that exist in the table\n    df = df[list(column_mapping.values())]\n\n    # Write to DB\n    df.to_sql(table_name, con=engine, if_exists=\"append\", index=False)\n    logger.info(f\"Loaded data into table: {table_name}\")\n\n\n# -----------------------------------------------------\n# Load CSV Data into Database Tables\n# -----------------------------------------------------\n\nfolder_path = \"data/*.csv\"\nfiles = glob.glob(folder_path)\nfiles = sorted(files, key=os.path.getmtime)\nbase_names = [path.splitext(path.basename(file))[0] for file in files]\n\nfor table in base_names:\n    try:\n        logger.info(f\"Loading data into table: {table}\")\n        load_csv_to_table(table, path.join(\"data/\", f\"{table}.csv\"))\n    except Exception as e:\n        logger.error(f\"Failed to ingest table {table}. Error: {e}\")\n        print(f\"Failed to ingest table {table}. Moving to the next!\")\n\nprint(\"Tables are populated.\")\n"
  requirements.txt: |-
    Faker==30.8.1
    loguru==0.7.2
    numpy==2.1.2
    pandas==2.2.3
    psycopg2==2.9.10
    python-dateutil==2.9.0.post0
    python-dotenv==1.0.1
    pytz==2024.2
    six==1.16.0
    SQLAlchemy==2.0.36
    typing_extensions==4.12.2
    tzdata==2024.2
kind: ConfigMap
metadata:
  labels:
    io.kompose.service: etl
  name: etl-cm0
